{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7685e3bc",
   "metadata": {},
   "source": [
    "# üß† Notebook 1: The Foundation - How LLMs Generate Text\n",
    "\n",
    "## Conference Event Description Generator Demo\n",
    "\n",
    "**Duration:** ~8 minutes  \n",
    "**Learning Objective:** Understand the fundamentals of how Large Language Models generate text token-by-token, and see this in action through a practical conference management use case.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What We'll Cover\n",
    "\n",
    "1. **LLM Theory Basics** - Tokens, transformers, and probability\n",
    "2. **Text Generation Process** - How models predict the next word\n",
    "3. **Practical Demo** - Building an event description generator\n",
    "4. **Parameters Impact** - Temperature, top-p, and creativity control\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ From Theory to Practice\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How LLMs work at a fundamental level\n",
    "- Why they sometimes \"hallucinate\" or make mistakes\n",
    "- How to control their creativity and consistency\n",
    "- Real business value through conference management automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873f7c7",
   "metadata": {},
   "source": [
    "## üß© Part 1: LLM Fundamentals - The Token Game\n",
    "\n",
    "Let's start with the basics. Every LLM works by:\n",
    "\n",
    "1. **Tokenization** - Breaking text into smaller pieces (tokens)\n",
    "2. **Context Understanding** - Looking at previous tokens to understand context\n",
    "3. **Prediction** - Calculating probabilities for the next token\n",
    "4. **Selection** - Choosing the next token based on those probabilities\n",
    "5. **Repeat** - Continue until complete\n",
    "\n",
    "### Think of it like autocomplete on steroids! \n",
    "\n",
    "When you type \"The conference will be held in...\" the model considers:\n",
    "- What locations make sense?\n",
    "- What style matches the context?\n",
    "- What information is most relevant?\n",
    "\n",
    "Then it predicts the most likely next words based on patterns learned from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2722d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Environment setup complete!\n",
      "üìä Plotting libraries ready\n",
      "üéØ Let's build our conference event generator!\n"
     ]
    }
   ],
   "source": [
    "# Let's start by setting up our environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Environment setup complete!\")\n",
    "print(\"üìä Plotting libraries ready\")\n",
    "print(\"üéØ Let's build our conference event generator!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Foundry connection established\n",
      "üîê Using secure managed identity authentication\n"
     ]
    }
   ],
   "source": [
    "# Set up Azure OpenAI for direct LLM access\n",
    "# This shows the foundational approach before we use agent frameworks\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://frederiekvandepitte4468-resource.cognitiveservices.azure.com/\")\n",
    "model_name = os.getenv(\"AZURE_OPENAI_MODEL\", \"gpt-4.1-nano\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4.1-nano\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"<your-api-key>\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        api_version=api_version,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    \n",
    "    # Test connection with a simple prompt\n",
    "    test_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Respond with just 'Connection successful!' to confirm setup.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Test connection\",\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=10,\n",
    "        temperature=0.0,\n",
    "        model=deployment\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Azure OpenAI connection established\")\n",
    "    print(f\"üéØ Using model: {model_name}\")\n",
    "    print(f\"\udce1 Endpoint: {endpoint}\")\n",
    "    print(f\"üîó Test response: {test_response.choices[0].message.content}\")\n",
    "    azure_client_ready = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Azure OpenAI setup needed: {e}\")\n",
    "    print(\"üí° Please check your .env file configuration\")\n",
    "    print(\"üìù For demo purposes, we'll use a simulation\")\n",
    "    azure_client_ready = False\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Simple Example: Direct LLM API Call\n",
    "# Let's see a basic example of how we talk to the LLM directly\n",
    "\n",
    "if azure_client_ready and client:\n",
    "    print(\"üî• Live Azure OpenAI Example:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    simple_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a conference marketing expert. Be concise and engaging.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Write a 2-sentence teaser for a session called 'Introduction to Kubernetes for Beginners'\",\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=100,\n",
    "        temperature=0.7,\n",
    "        model=deployment\n",
    "    )\n",
    "    \n",
    "    print(\"‚ú® AI Response:\")\n",
    "    print(simple_response.choices[0].message.content)\n",
    "    print()\n",
    "    print(\"üéØ This is the foundation - simple prompt ‚Üí AI response!\")\n",
    "    \n",
    "else:\n",
    "    print(\"üß™ Simulated Example (Azure OpenAI not configured):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"‚ú® AI Response:\")\n",
    "    print(\"Ready to dive into container orchestration? Join our beginner-friendly Kubernetes session where you'll learn to deploy, scale, and manage applications with confidence. Walk away with hands-on skills and the knowledge to streamline your development workflow!\")\n",
    "    print()\n",
    "    print(\"üéØ This is what a real API call would return!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36422d0",
   "metadata": {},
   "source": [
    "## üé® Part 2: Text Generation Parameters - The Creativity Controls\n",
    "\n",
    "Before we generate our first event description, let's understand the key parameters that control how creative or conservative our AI becomes:\n",
    "\n",
    "### üå°Ô∏è **Temperature** (0.0 - 2.0)\n",
    "- **Low (0.0-0.3)**: Predictable, consistent, factual\n",
    "- **Medium (0.4-0.7)**: Balanced creativity and reliability  \n",
    "- **High (0.8-2.0)**: Creative, varied, sometimes unpredictable\n",
    "\n",
    "### üéØ **Top-p** (0.0 - 1.0)  \n",
    "- Controls vocabulary diversity\n",
    "- 0.1 = Very focused, limited word choices\n",
    "- 0.9 = Diverse vocabulary, more creative\n",
    "\n",
    "### üé≤ **Max Tokens**\n",
    "- Maximum length of generated text\n",
    "- Helps control response length and cost\n",
    "\n",
    "### Why This Matters for Conference Management:\n",
    "- **Event descriptions**: Medium creativity for engaging but accurate content\n",
    "- **Speaker bios**: Low temperature for factual accuracy\n",
    "- **Marketing copy**: Higher creativity for compelling messaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Event Description Generator Ready!\n",
      "üìù Test session data loaded\n",
      "üöÄ Ready to demonstrate different creativity levels!\n"
     ]
    }
   ],
   "source": [
    "# Demo: Generate event descriptions with different creativity levels\n",
    "def generate_event_description(session_info: Dict, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate a conference session description using Azure OpenAI directly\n",
    "    \n",
    "    Args:\n",
    "        session_info: Dictionary containing session details\n",
    "        temperature: Controls creativity (0.0 = conservative, 1.0 = creative)\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert conference event description generator.\n",
    "    Create engaging, professional, and informative descriptions.\n",
    "    Focus on value proposition and clear outcomes for attendees.\n",
    "    Keep descriptions between 150-300 words.\n",
    "    Always include practical takeaways.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Create an engaging conference session description for:\n",
    "    \n",
    "    Title: {session_info['title']}\n",
    "    Speaker: {session_info['speaker']}\n",
    "    Duration: {session_info['duration']} minutes\n",
    "    Track: {session_info['track']}\n",
    "    Level: {session_info['level']}\n",
    "    Key Topics: {', '.join(session_info['topics'])}\n",
    "    \n",
    "    Include:\n",
    "    - Compelling overview that highlights value\n",
    "    - What attendees will learn\n",
    "    - Practical takeaways\n",
    "    - Who should attend\n",
    "    \n",
    "    Style: Professional but engaging, suitable for technical conference\n",
    "    \"\"\"\n",
    "    \n",
    "    if azure_client_ready and client:\n",
    "        try:\n",
    "            # Use real Azure OpenAI model\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                max_completion_tokens=500,\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0,\n",
    "                model=deployment\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è API call failed: {e}\")\n",
    "            return generate_simulation(session_info, temperature)\n",
    "    else:\n",
    "        # Simulation for demo purposes\n",
    "        return generate_simulation(session_info, temperature)\n",
    "\n",
    "def generate_simulation(session_info: Dict, temperature: float) -> str:\n",
    "    \"\"\"Generate a simulated response for demo purposes\"\"\"\n",
    "    creativity_styles = {\n",
    "        0.2: \"straightforward and factual\",\n",
    "        0.7: \"engaging and professional\", \n",
    "        1.2: \"creative and dynamic\"\n",
    "    }\n",
    "    \n",
    "    style_desc = creativity_styles.get(temperature, \"balanced\")\n",
    "    \n",
    "    return f\"\"\"**{session_info['title']}**\n",
    "\n",
    "Join {session_info['speaker']} for an intensive {session_info['duration']}-minute deep dive into cutting-edge {session_info['track']} technologies. This {session_info['level']}-level session will cover {', '.join(session_info['topics'])}, providing you with practical insights you can implement immediately.\n",
    "\n",
    "**What You'll Learn:**\n",
    "‚Ä¢ Advanced techniques in {session_info['topics'][0]}\n",
    "‚Ä¢ Real-world implementation strategies  \n",
    "‚Ä¢ Best practices from industry leaders\n",
    "‚Ä¢ Hands-on examples and case studies\n",
    "\n",
    "**Who Should Attend:**\n",
    "Perfect for developers, architects, and technical leaders looking to stay ahead of the curve in {session_info['track']}.\n",
    "\n",
    "**Takeaways:**\n",
    "Leave with actionable knowledge and a clear roadmap for implementing these technologies in your organization.\n",
    "\n",
    "*[Simulated response - {style_desc} style, temperature: {temperature}]*\n",
    "\"\"\"\n",
    "\n",
    "# Test session data\n",
    "test_session = {\n",
    "    \"title\": \"Building Resilient Microservices with Event-Driven Architecture\",\n",
    "    \"speaker\": \"Dr. Sarah Chen\",\n",
    "    \"duration\": 45,\n",
    "    \"track\": \"Architecture\",\n",
    "    \"level\": \"Intermediate\",\n",
    "    \"topics\": [\"Event Sourcing\", \"CQRS\", \"Message Queues\", \"Distributed Systems\"]\n",
    "}\n",
    "\n",
    "print(\"üéØ Event Description Generator Ready!\")\n",
    "print(\"üìù Test session data loaded\")\n",
    "print(\"üöÄ Ready to demonstrate different creativity levels!\")\n",
    "print(f\"üîß Using direct Azure OpenAI API calls (not agent framework yet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7705f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CONSERVATIVE (Temperature: 0.2)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error from Azure AI API: API key is required                                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Error from Azure AI API: API key is required                                                              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Attempt <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> failed: API key is required                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING \u001b[0m Attempt \u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m failed: API key is required                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Failed after <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> attempts. Last error using <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AzureAIFoundry</span><span style=\"font-weight: bold\">(</span>Phi-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Failed after \u001b[1;36m1\u001b[0m attempts. Last error using \u001b[1;35mAzureAIFoundry\u001b[0m\u001b[1m(\u001b[0mPhi-\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModelProviderError",
     "evalue": "API key is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/azure/ai_foundry.py:218\u001b[39m, in \u001b[36mAzureAIFoundry.invoke\u001b[39m\u001b[34m(self, messages, assistant_message, response_format, tools, tool_choice, run_response)\u001b[39m\n\u001b[32m    217\u001b[39m assistant_message.metrics.start_timer()\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m provider_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.complete(\n\u001b[32m    219\u001b[39m     messages=[format_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],\n\u001b[32m    220\u001b[39m     **\u001b[38;5;28mself\u001b[39m.get_request_params(tools=tools, response_format=response_format, tool_choice=tool_choice),\n\u001b[32m    221\u001b[39m )\n\u001b[32m    222\u001b[39m assistant_message.metrics.stop_timer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/azure/ai_foundry.py:168\u001b[39m, in \u001b[36mAzureAIFoundry.get_client\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m client_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_client_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m.client = ChatCompletionsClient(**client_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/azure/ai_foundry.py:138\u001b[39m, in \u001b[36mAzureAIFoundry._get_client_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAPI key is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.azure_endpoint:\n",
      "\u001b[31mValueError\u001b[39m: API key is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelProviderError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müî• CONSERVATIVE (Temperature: 0.2)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m conservative_desc = \u001b[43mgenerate_event_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(conservative_desc)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mgenerate_event_description\u001b[39m\u001b[34m(session_info, temperature)\u001b[39m\n\u001b[32m     11\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33mCreate an engaging conference session description for:\u001b[39m\n\u001b[32m     13\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33mStyle: Professional but engaging, suitable for technical conference\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m azure_model:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Use real Azure AI Foundry model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     response = \u001b[43mevent_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.content\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Simulation for demo purposes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/agent/agent.py:1742\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, input, stream, stream_events, stream_intermediate_steps, user_id, session_id, session_state, audio, images, videos, files, retries, knowledge_filters, add_history_to_context, add_dependencies_to_context, add_session_state_to_context, dependencies, metadata, yield_run_response, debug_mode, **kwargs)\u001b[39m\n\u001b[32m   1739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m   1740\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generator_wrapper(create_run_error_event(run_response, error=\u001b[38;5;28mstr\u001b[39m(last_exception)))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m last_exception\n\u001b[32m   1743\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1744\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/agent/agent.py:1688\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, input, stream, stream_events, stream_intermediate_steps, user_id, session_id, session_state, audio, images, videos, files, retries, knowledge_filters, add_history_to_context, add_dependencies_to_context, add_session_state_to_context, dependencies, metadata, yield_run_response, debug_mode, **kwargs)\u001b[39m\n\u001b[32m   1686\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response_iterator\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m            \u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m            \u001b[49m\u001b[43mknowledge_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m            \u001b[49m\u001b[43madd_history_to_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m            \u001b[49m\u001b[43madd_dependencies_to_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_dependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m            \u001b[49m\u001b[43madd_session_state_to_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_session_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_dependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1703\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (InputCheckError, OutputCheckError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/agent/agent.py:1043\u001b[39m, in \u001b[36mAgent._run\u001b[39m\u001b[34m(self, run_response, session, session_state, user_id, knowledge_filters, add_history_to_context, add_dependencies_to_context, add_session_state_to_context, metadata, response_format, dependencies, debug_mode, **kwargs)\u001b[39m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# 6. Generate a response from the Model (includes running function calls)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28mself\u001b[39m.model = cast(Model, \u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m model_response: ModelResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43msend_media_to_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_media_to_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# Check for cancellation after model call\u001b[39;00m\n\u001b[32m   1054\u001b[39m raise_if_cancelled(run_response.run_id)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/base.py:350\u001b[39m, in \u001b[36mModel.response\u001b[39m\u001b[34m(self, messages, response_format, tools, tool_choice, tool_call_limit, run_response, send_media_to_model)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;66;03m# Get response from model\u001b[39;00m\n\u001b[32m    349\u001b[39m     assistant_message = Message(role=\u001b[38;5;28mself\u001b[39m.assistant_message_role)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tool_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# Add assistant message to messages\u001b[39;00m\n\u001b[32m    361\u001b[39m     messages.append(assistant_message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/base.py:656\u001b[39m, in \u001b[36mModel._process_model_response\u001b[39m\u001b[34m(self, messages, assistant_message, model_response, response_format, tools, tool_choice, run_response)\u001b[39m\n\u001b[32m    649\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[33;03mProcess a single model response and return the assistant message and whether to continue.\u001b[39;00m\n\u001b[32m    651\u001b[39m \n\u001b[32m    652\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    653\u001b[39m \u001b[33;03m    Tuple[Message, bool]: (assistant_message, should_continue)\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m provider_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# Populate the assistant message\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;28mself\u001b[39m._populate_assistant_message(assistant_message=assistant_message, provider_response=provider_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/fvandepitte/talks/from-completion-to-agenticai/venv/lib/python3.13/site-packages/agno/models/azure/ai_foundry.py:238\u001b[39m, in \u001b[36mAzureAIFoundry.invoke\u001b[39m\u001b[34m(self, messages, assistant_message, response_format, tools, tool_choice, run_response)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    237\u001b[39m     log_error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError from Azure AI API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProviderError(message=\u001b[38;5;28mstr\u001b[39m(e), model_name=\u001b[38;5;28mself\u001b[39m.name, model_id=\u001b[38;5;28mself\u001b[39m.id) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mModelProviderError\u001b[39m: API key is required"
     ]
    }
   ],
   "source": [
    "# üé≠ Demo: Compare different creativity levels\n",
    "\n",
    "print(\"üî• CONSERVATIVE (Temperature: 0.2)\")\n",
    "print(\"=\" * 50)\n",
    "conservative_desc = generate_event_description(test_session, temperature=0.2)\n",
    "print(conservative_desc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"‚öñÔ∏è BALANCED (Temperature: 0.7)\")\n",
    "print(\"=\" * 50)\n",
    "balanced_desc = generate_event_description(test_session, temperature=0.7)\n",
    "print(balanced_desc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"üé® CREATIVE (Temperature: 1.2)\")\n",
    "print(\"=\" * 50)\n",
    "creative_desc = generate_event_description(test_session, temperature=1.2)\n",
    "print(creative_desc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"üéØ Key Observations:\")\n",
    "print(\"‚Ä¢ Conservative: Factual, predictable, safe for official use\")\n",
    "print(\"‚Ä¢ Balanced: Engaging while maintaining professionalism\") \n",
    "print(\"‚Ä¢ Creative: More varied language, higher marketing appeal\")\n",
    "print(\"‚Ä¢ Choose temperature based on your conference's brand and audience!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf3fea",
   "metadata": {},
   "source": [
    "## üìä Part 3: Understanding Token-by-Token Generation\n",
    "\n",
    "Let's visualize how LLMs actually build text one token at a time. This helps explain why they sometimes make mistakes or \"hallucinate\"!\n",
    "\n",
    "### The Process:\n",
    "1. **Start with prompt** ‚Üí Model reads your input\n",
    "2. **Calculate probabilities** ‚Üí For every possible next token  \n",
    "3. **Sample based on temperature** ‚Üí Choose next token\n",
    "4. **Add to context** ‚Üí Token becomes part of the story so far\n",
    "5. **Repeat** ‚Üí Until completion or max tokens reached\n",
    "\n",
    "### Why This Matters:\n",
    "- **Consistency**: Each token only \"sees\" what came before\n",
    "- **Context limits**: Models have finite memory windows  \n",
    "- **Probability**: Sometimes unlikely (but valid) tokens get selected\n",
    "- **Hallucination**: Model confidently predicts plausible but incorrect information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate token-by-token generation process\n",
    "def simulate_token_generation(prompt: str, max_tokens: int = 50):\n",
    "    \"\"\"\n",
    "    Simulate how an LLM generates text token by token\n",
    "    This is a simplified simulation for educational purposes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated token probabilities for conference-related text\n",
    "    conference_tokens = [\n",
    "        \"conference\", \"session\", \"attendees\", \"speakers\", \"workshop\",\n",
    "        \"presentation\", \"networking\", \"innovation\", \"technology\", \"insights\",\n",
    "        \"practical\", \"hands-on\", \"experience\", \"learning\", \"industry\",\n",
    "        \"will\", \"learn\", \"explore\", \"discover\", \"master\", \"understand\"\n",
    "    ]\n",
    "    \n",
    "    general_tokens = [\n",
    "        \"the\", \"and\", \"to\", \"of\", \"in\", \"for\", \"with\", \"on\", \"at\", \"by\",\n",
    "        \"this\", \"that\", \"these\", \"those\", \"you\", \"your\", \"our\", \"we\"\n",
    "    ]\n",
    "    \n",
    "    tokens_generated = []\n",
    "    current_text = prompt\n",
    "    \n",
    "    print(f\"üöÄ Starting generation from: '{prompt}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for step in range(max_tokens):\n",
    "        # Simulate probability calculation based on context\n",
    "        if len(tokens_generated) < 5:\n",
    "            # Early tokens more likely to be general structure words\n",
    "            candidates = general_tokens + conference_tokens[:5]\n",
    "        else:\n",
    "            # Later tokens more domain-specific\n",
    "            candidates = conference_tokens + general_tokens[:8]\n",
    "        \n",
    "        # Simulate token selection (simplified)\n",
    "        import random\n",
    "        next_token = random.choice(candidates)\n",
    "        \n",
    "        tokens_generated.append(next_token)\n",
    "        current_text += \" \" + next_token\n",
    "        \n",
    "        # Show progress every few tokens\n",
    "        if step % 8 == 0 or step < 5:\n",
    "            print(f\"Step {step+1:2d}: '{next_token}' ‚Üí {current_text}\")\n",
    "            \n",
    "        # Simple stopping condition\n",
    "        if next_token in [\".\", \"!\", \"?\"] and len(tokens_generated) > 10:\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Generated {len(tokens_generated)} tokens\")\n",
    "    print(f\"üìù Final text: {current_text}\")\n",
    "    \n",
    "    return tokens_generated, current_text\n",
    "\n",
    "# Demo the process\n",
    "print(\"üß™ Simulating Token-by-Token Generation\")\n",
    "print(\"(Simplified for demonstration - real models are much more sophisticated)\")\n",
    "print()\n",
    "\n",
    "tokens, final_text = simulate_token_generation(\"This conference session about AI will\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c7038",
   "metadata": {},
   "source": [
    "## üè¢ Part 4: Business Value - From Theory to Practice\n",
    "\n",
    "Now that you understand how LLMs work, let's see the **real business impact** for conference management:\n",
    "\n",
    "### ‚è∞ **Time Savings**\n",
    "- **Before**: 30-45 minutes per session description\n",
    "- **After**: 2-3 minutes with AI assistance\n",
    "- **Impact**: 90%+ time reduction for content creation\n",
    "\n",
    "### ‚ú® **Quality Consistency**  \n",
    "- Standardized tone and structure across all sessions\n",
    "- No more writer's block or inconsistent messaging\n",
    "- Professional quality even for last-minute additions\n",
    "\n",
    "### üéØ **Scalability**\n",
    "- Generate descriptions for 100+ sessions in minutes\n",
    "- Easy localization for international conferences\n",
    "- Rapid iteration and A/B testing of messaging\n",
    "\n",
    "### üí° **Creative Enhancement**\n",
    "- AI suggests angles you might not have considered  \n",
    "- Helps non-writers create compelling content\n",
    "- Maintains engagement while ensuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Interactive Demo: Build Your Own Session Description\n",
    "\n",
    "def interactive_session_builder():\n",
    "    \"\"\"\n",
    "    Interactive tool to create conference session descriptions\n",
    "    Demonstrates the practical value of LLM text generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üé™ Conference Session Description Builder\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Let's create a compelling session description together!\")\n",
    "    print()\n",
    "    \n",
    "    # In a real demo, these would be interactive inputs\n",
    "    # For notebook purposes, we'll use example data\n",
    "    \n",
    "    sample_sessions = [\n",
    "        {\n",
    "            \"title\": \"Kubernetes in Production: Lessons from the Trenches\",\n",
    "            \"speaker\": \"Alex Rodriguez\", \n",
    "            \"duration\": 60,\n",
    "            \"track\": \"DevOps\",\n",
    "            \"level\": \"Advanced\",\n",
    "            \"topics\": [\"Container Orchestration\", \"Production Deployments\", \"Monitoring\", \"Scaling\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Introduction to Machine Learning for Web Developers\", \n",
    "            \"speaker\": \"Dr. Maya Patel\",\n",
    "            \"duration\": 45,\n",
    "            \"track\": \"AI/ML\",\n",
    "            \"level\": \"Beginner\", \n",
    "            \"topics\": [\"Neural Networks\", \"TensorFlow.js\", \"Browser ML\", \"Practical Applications\"]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Building Accessible React Components\",\n",
    "            \"speaker\": \"Jordan Kim\",\n",
    "            \"duration\": 30, \n",
    "            \"track\": \"Frontend\",\n",
    "            \"level\": \"Intermediate\",\n",
    "            \"topics\": [\"ARIA\", \"Screen Readers\", \"Keyboard Navigation\", \"Inclusive Design\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, session in enumerate(sample_sessions, 1):\n",
    "        print(f\"üìã Session #{i}: {session['title']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Generate description with optimal settings for conference content\n",
    "        description = generate_event_description(session, temperature=0.6)\n",
    "        print(description)\n",
    "        print()\n",
    "        \n",
    "        # Show the efficiency gain\n",
    "        print(f\"‚ö° Generated in ~2 seconds vs ~30 minutes manual writing\")\n",
    "        print(f\"üéØ Consistent quality and structure\")\n",
    "        print(f\"üìà Ready for immediate use in conference program\")\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "\n",
    "# Run the interactive demo\n",
    "interactive_session_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c89c5",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways - The Foundation is Set!\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **üß† LLM Fundamentals**\n",
    "   - Text generation is probabilistic, not deterministic\n",
    "   - Models predict one token at a time based on context\n",
    "   - Understanding this helps explain AI behavior and limitations\n",
    "\n",
    "2. **üéöÔ∏è Parameter Control**\n",
    "   - Temperature controls creativity vs consistency\n",
    "   - Lower values for factual content, higher for creative writing\n",
    "   - Choose settings based on your specific business needs\n",
    "\n",
    "3. **üíº Business Impact**\n",
    "   - 90%+ time reduction in content creation\n",
    "   - Consistent quality and professional tone\n",
    "   - Scalable solution for large-scale events\n",
    "\n",
    "4. **üîß Direct API Implementation**\n",
    "   - Started with raw Azure OpenAI API calls\n",
    "   - Simple message format: system prompt + user prompt\n",
    "   - Direct control over all parameters (temperature, max_tokens, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "In **Notebook 2**, we'll evolve from direct API calls to **conversational AI**:\n",
    "- Memory and context management\n",
    "- Building conference chatbots with AGNO AI framework\n",
    "- Handling complex multi-turn conversations\n",
    "\n",
    "**The journey from completion to conversation begins!** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
